Notes from the call with Elliot from AISI
Scope: sectors, years, and focus
Sectors / framing
Strong interest in coverage across the whole economy, but:
Particular focus on CNI (Critical National Infrastructure) sectors (12–13 UK CNI sectors) as a way to structure analysis and prioritization.
Years
Most value is post‑2021 (roughly “post‑ChatGPT moment” / transformative AI focus).
Pre‑2021 mainly useful as a baseline / trend context, not the main focus.
Risk focus
AISI is more interested in systemic misuse / operational risks than very long‑term “gradual disempowerment” scenarios for this dataset.
Examples Elliot cares about:
Cyber attacks, fraud, systemic misuse
Over‑reliance, single points of failure, concentration of tools/vendors
Signals that multiple companies in a sector are adopting the same tools / architectures.
What to classify (taxonomy directions)
Technology type / AI category
Priority: general‑purpose models / GenAI / LLMs / agents.
Classic ML / predictive modelling is much lower institutional priority, though still informative for context.
You should try to distinguish:
Traditional ML vs
LLM‑type tools vs
More agentic / autonomous systems
…but Elliot expects this to be imperfect and often inferred from use‑case wording.
Adoption dimensions (3 axes)
Type of AI used
Traditional ML vs LLMs vs agents / agentic systems.
Depth / intensity of use
Pilots / experiments vs broad deployment (e.g. Copilot for whole org) vs deeply embedded in core processes.
Criticality to operations
Peripheral uses (e.g. customer support bots) vs core infrastructure / core decision‑making (e.g. underwriting, risk management, key operational systems).
Also: adoption by the company vs by its customers, where possible.
Risk taxonomy
Elliot will try to dig up a government risk register (or similar) and share an Excel risk taxonomy if/when he can.
He’s aware there may be mismatch between:
Rational / policy‑oriented risk categories, and
How companies actually phrase and group risks in reports.
Open design question to include in the choices report:
Stick closely to AC’s preferred taxonomy vs
Pivot partly to emergent categories from the text.
Validation and correspondences
Ground truth vs reports
Elliot is not asking for a big, expensive ground‑truth validation exercise.
Preference:
Light “vibe check” using a small number of known incidents / third‑party data.
Main focus on internal consistency and cross‑sector/time comparability rather than perfect real‑world calibration.
Models
At least two different model families on a sample.
If outputs are very similar, no need to test many more; big divergences would justify more digging.
Usefulness / boilerplate / severity
Boilerplate vs substantive
Very interested in distinguishing:
Generic, lawyer‑driven boilerplate vs
Specific, evidenced risk statements (with mitigation, numbers, etc.).
Severity and likelihood
If companies explicitly rate severity/likelihood, this is very valuable:
If they say high and AC agrees → an easy lever for engagement with regulators/departments.
If they say low and AC disagrees → evidence that firms are under‑estimating risks.
Lack of mention
Three useful scenarios:
Risk is mentioned as high → strong signal, matches AC concern.
Risk is mentioned as low but AC thinks it’s high → mis‑calibration to highlight.
Risk not mentioned at all in a sector where AC thinks it should be → evidence of blind spots.
He doesn’t yet have a definitive list of “must‑mention” risks but wants you to set things up so it’s easy to interrogate these patterns later.
Visualization preferences
High‑value “starter” views:
Risk trends over time (all sectors / selected sectors).
Risk trends by sector (e.g. stacked charts).
Heatmaps / tables:
For a given year, a grid: sectors × risk types, with intensity counts or scores.
Potentially multiple such grids side‑by‑side for different years to visually track emerging “hot spots”.
Adoption/adoption‑intensity by sector, analogous to the risk views.
He prefers simple, legible dashboards over a single very complex all‑in‑one interface.
Ability to select a risk type and see its distribution across sectors would be very helpful; comparison of two risks on the same chart is “nice‑to‑have”.
Process & expectations
Elliot is happy with:
Iterative questionnaires from you (10–15 questions at a time).
Answering some now, some later, and taking some back to his wider team.
His standard: not academic‑paper‑grade, but:
“Robust enough that I can tell Ofgem and they don’t laugh at me.”
Coverage / breadth is more important than hyper‑precise estimates on a tiny sample (he explicitly leans toward “less robust on more” rather than “ultra‑robust on very little”).

