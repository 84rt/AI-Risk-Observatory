mention_type: |
  You are an expert analyst labelling AI mentions from company annual reports.

  ## CONTEXT
  Company: {firm_name}
  Sector: {sector}
  Report Year: {report_year}
  Report Section: {report_section}

  ## TASK
  Classify the excerpt into zero or more mention types. Mention types are NOT mutually exclusive.
  If excerpt is a false positive (no AI mention), return the "none" tag.
  Add confidence scores only for the tags in mention_types. 
  The excerpts often contain multiple unrelated sentences. Make sure that the tags you assign are only to the sentences talking about AI, not the excerpt as a whole.

  ## EXAMPLES
  - "We deployed an AI chatbot for customer support." -> adoption ~0.9
  - "AI could increase misinformation risks. Deepfakes have significantly impaired our value proposition" -> risk ~0.8, harm ~0.8
  - "We are exploring AI opportunities." -> general_ambiguous ~0.7
  - "Automation of customer service tasks improved our..." -> general_ambiguous ~0.2
  - "Our office in Shangh'ai' has opp..." -> none ~0.9
  
  ## MENTION TYPES (DEFINITIONS)
  - adoption: Concrete mention of deployment, use, rollout, pilot, or implementation of AI within the company
    or for their clients. Mentions of internal AI tooling or customer-facing systems. Adoption requires explicit company-specific language (we/our/our clients) or an explicit implementation/pilot; otherwise use general_ambiguous. If the mention is vague assign it very low confidence.
  - risk: AI described as a risk, or a material concern to the company (i.e. legal, cybersecurity, operational, reputational, or regulatory risk directly related to AI). Make sure to only assign this tag if the risk is directly related to AI, the probability of an unrelated other risk mention in the excerpt is high. 
    security, operational, reputational, or regulatory risk).
  - harm: AI discussed as causing harm or enabling harm (misinformation, fraud, cyber abuse,
    safety incidents, discrimination, etc.).
  - vendor: Explicit mention of an AI vendor or platform (Microsoft, Google, OpenAI, AWS, etc.)
    or a named third-party provider of AI capabilities.
  - general_ambiguous: High-level plans, opportunities, strategy, or vague statements that do
    not clearly fit adoption, risk, harm, or vendor.
    If AI is explicitly mentioned but does not meet adoption/risk/harm/vendor, use general_ambiguous.

  ## CONFIDENCE GUIDANCE (0.0-1.0)
  - 0.0: confident NO — clear absence of this mention type
  - 0.2: faint/implicit signal; could be this type but hard to tell
  - 0.5: highly uncertain — no strong evidence either way
  - 0.8: likely YES — strong signal, but not fully explicit
  - 1.0: confident YES — explicit, unambiguous mention

mention_type_v2: |
  You are an expert analyst labeling AI mentions from company annual reports.

  ## CONTEXT
  Company: {firm_name}
  Sector: {sector}
  Report Year: {report_year}
  Report Section: {report_section}

  ## TASK
  Assign ALL mention types that apply to the excerpt. Types are NOT mutually exclusive.
  If the excerpt contains no AI mention, return only "none".
  Only tag content that is explicitly about AI in the excerpt; ignore unrelated sentences.
  You may be instructed to include or omit a brief "reasoning" field in the JSON output.
  {reasoning_instruction}

  ## RULES (STRICT)
  - AI EXPLICITNESS GATE (HARD RULE): If the excerpt does NOT explicitly mention AI/ML/LLM/GenAI
    or a clearly AI-specific technique (e.g., machine learning, neural networks, computer vision),
    return "none". Terms like "data analytics", "automation", "digital tools", or "advanced analytics"
    alone are NOT AI.
  - adoption: must describe real deployment, implementation, rollout, pilot, or use of AI
    by the company or for its clients. Generic intent/strategy/roadmaps = NOT adoption.
    Treat "exploring", "piloting", or "investigating" AI use as adoption ONLY when
    it refers to specific initiatives underway (e.g., a pilot, trial, or program).
    If it's general interest or high-level consideration, use general_ambiguous.
    Delivering AI systems for clients counts as adoption; pure consulting/advice without
    deployment does NOT.
  - risk: must describe AI as a risk or downside (legal, operational, reputational, cybersecurity,
    regulatory) directly tied to AI. Generic risk language without AI = NOT risk.
  - harm: must describe AI causing or enabling harm (misinformation, fraud, abuse, safety incidents).
  - vendor: must explicitly name a third-party AI vendor/platform (e.g., Microsoft, Google, OpenAI, AWS).
  - general_ambiguous: vague AI strategy, high-level plans, or non-specific AI mentions.
    If AI is explicitly mentioned but does not meet adoption/risk/harm/vendor, use general_ambiguous.
    general_ambiguous should NOT co-occur with other labels.
  - none: no AI mention, false positive, or unrelated automation not clearly AI.
  - confidence scores always indicate how likely the label applies (including "none").
  - include a label in mention_types only if confidence >= 0.2.

  ## EXAMPLES
  - "We deployed an AI chatbot for customer support." -> adoption ~0.9
  - "We are exploring AI opportunities." -> general_ambiguous ~0.7
  - "We are piloting AI to automate invoice processing." -> adoption ~0.8
  - "We use predictive analytics to optimize routing." -> none (unless AI/ML explicitly stated)
  - "We use data analytics to improve forecasting and operations." -> none
  - "We applied data analytics in audit testing with no AI reference." -> none
  - "AI could increase misinformation risks." -> risk ~0.8, harm ~0.7
  - "AI is a long-term megatrend; we are evaluating this risk." -> risk ~0.7 (not adoption)
  - "We partner with Microsoft for AI tooling." -> vendor ~0.9, adoption ~0.6
  - "We deployed OpenAI models for customer support." -> vendor ~0.9, adoption ~0.9
  - "We delivered AI systems for clients in 2024." -> adoption ~0.8
  - "Automation of customer service tasks improved our..." -> general_ambiguous ~0.2 (not necessarily AI)
  - "Our office in Shangh'ai' has opp..." -> none ~0.9

  ## OUTPUT CONSTRAINTS
  - mention_types must be non-empty.
  - If "none" is present, it must be the only label.
  - Provide a confidence score for EVERY label in mention_types.
  - Do NOT include confidence scores for labels not in mention_types.

  ## CONFIDENCE GUIDANCE (0.0-1.0)
  - 0.2: faint/implicit signal; could be this type but hard to tell
  - 0.5: uncertain — weak evidence
  - 0.8: likely YES — strong signal, but not fully explicit
  - 0.95-1.0: confident YES — explicit, unambiguous mention

mention_type_v3: |
  You are an expert analyst labeling AI mentions from company annual reports.

  ## TASK
  Assign ALL mention types that apply to the excerpt. Types are NOT mutually exclusive except for "none".
  If the excerpt contains no AI mention, return only "none". Only tag content that is explicitly about AI in the excerpt; ignore unrelated sentences.
  {reasoning_instruction}

  ## RULES
  1. AI EXPLICITNESS GATE: If the excerpt does NOT explicitly mention AI/ML/LLM/GenAI or a clearly AI-specific technique (e.g., machine learning, neural networks, computer vision), assign the tag "none". Terms like "data analytics" or "digital tools" generally are NOT considered AI under our definition. The tag "none" is used when there is no AI mention, a false positive, or unrelated automation not clearly AI. Only consider terms like "autonomus or virtual assistant" as AI if it can be clearly attributed to AI. Otherwise, use the following tags in a non-mutually exclusive manner: adoption (the current usage of AI technology by the company), risk (AI as a risk: risks that are directly coming from AI), harm (past harms that were caused by AI), vendor (any mention of a provider of AI technology), general_ambiguous (any statement about AI that does not fit into the other tags). Here are more details on each tag:
    - adoption: must directly describe real current deployment, implementation, rollout, pilot, or use of AI by the company or for its clients. Generic statements about intent/strategy/roadmaps/plans (adoption in the future) are NOT considered adoption. Treat "exploring", "piloting", or "investigating" AI use as adoption ONLY when it refers to initiative currently underway (e.g. "current trail resulted in..."). Delivering AI systems directly or indirectly for clients does count as adoption; pure consulting/advice without deployment does NOT.
    - risk: must directly attribute AI as the source of a risk or downside (i.e. strategic & competitive, operational & technical, cybersecurity, workforce impacts, regulatory & compliance, information integrity, reputational & ethical, third-party & supply chain, environmental impact, and national security etc.). The excerpt might contain a sentence on risk and a separate sentence on AI; make sure to only assign the "risk" tag if AI is mentioned as the source of the risk. Generic risk language without explicitly mentioning AI is NOT risk from AI. However, the risk section might outline downstream risks or effects from AI technologies in an indirecty way, these should be classified as risk from AI.
    - harm: must describe past harms that were caused by AI (misinformation, fraud, abuse, safety incidents).
    - vendor: must explicitly name a third-party AI vendor/platform that provices the AI technology (i.e. Microsoft, Google, OpenAI, AWS, or explicitly developed in-house). We primarely want to tag text that mentions any information about what AI models are used by the comapny (i.e. GPT or Google Gemini).
    - general_ambiguous: vague AI strategy, high-level plans, or AI mentions that don't have enough context or are not specific enough. If AI is explicitly mentioned but does not meet adoption/risk/harm/vendor, use general_ambiguous. The tag general_ambiguous should only be added when the excerpt clearly talks about AI but does not meet the other tag definitions.
  2. Assign confidence scores to each tag. Confidence scores always indicate how likely the label applies (including "none").

  ## CONFIDENCE GUIDANCE (0.0-1.0)
  - 0.2: faint/implicit signal; could be this type but hard to tell
  - 0.5: uncertain — weak evidence
  - 0.8: likely YES — strong signal, but not fully explicit
  - 0.95-1.0: confident YES — explicit, unambiguous mention

  ## EXAMPLES
  - "We deployed an AI chatbot for customer support." -> adoption ~0.9
  - "We are exploring AI opportunities." -> general_ambiguous ~0.7
  - "We are piloting AI to automate invoice processing." -> adoption ~0.8
  - "We use data analytics and predictive analytics to optimize routing." -> none ~0.6 (unless AI/ML explicitly stated)
  - "AI could increase misinformation risks." -> risk ~0.8
  - "AI is a long-term megatrend, being widely adopted within the industry; we are evaluating any risks associated with it." -> risk ~0.7 (no "adoption" tag, as no evidence of adoption by company)
  - "We partner with Microsoft for AI tooling." -> vendor ~0.9, adoption ~0.6
  - "We partnered with OpenAI to deliver AI systems for clients in 2024." -> vendor ~0.9, adoption ~0.8
  - "Automation of customer service tasks improved our..." -> general_ambiguous ~0.2 (not necessarily AI)
  - "Address: FT-AI 4810 Shangh'ai', is where the..." -> none ~0.9 (a false positive)
  - "AI-generated misinformation has damaged our brand reputation." -> harm ~0.9

  ## OUTPUT CONSTRAINTS
  - mention_types must be non-empty.
  - If "none" is present, it must be the only label.
  - Provide a confidence score for EVERY label in mention_types.
  - Do NOT include confidence scores for labels not in mention_types.

  ## EXCERPT CONTEXT
  Company: {firm_name} | Sector: {sector} | Report Year: {report_year} | Report Section: {report_section}

adoption_type: |
  You are an expert analyst classifying the type of AI adoption in company annual reports.

  ## TASK
  The excerpt has already been classified with the following mention types: {mention_types}.
  Classify the AI adoped by the company into type(s) into 3 categories. Categories are NOT mutually exclusive.
  If the mention does not discuss current AI adoption, even indirectly, classify all categories as 0.
  Assign signal scores (0-3) for ALL adoption types. Use 0 for types not present.
  {reasoning_instruction}

  ## CATEGORIES
  - non_llm: Traditional AI/ML — everything that is AI but not LLM or Agentic AI (e.g., computer vision, predictive analytics, fraud detection, recommendation engines, anomaly detection, robotic process automation with ML).
  - llm: Large Language Models — GPT, ChatGPT, Gemini, Claude, Copilot, text generation, NLP chatbots, document summarization, code generation or anything in the GenAI category.
  - agentic: Autonomous/agentic AI — self-directed AI agents that perform tasks with limited human intervention, or AI systems replacing human decision-making workflows end-to-end. These can often be LLM-based, so when stated explicitly select both.

  ## RULES
  1. If the mention is falsely classified as adoption—i.e., it does not clearly talk about the company's *current* usage of AI (only talks about plans or opportunities without any evidence for current deployment), or there isn't enough information in the mention to make a proper categorization, set all three signals to 0.
  2. A single excerpt can describe multiple adoption types (e.g., LLM + agentic). But some can be more evident than others, so make sure to assign the appropriate signal scores to each tag when assigning multiple.
  3. If the excerpt is vague about which type, assign low signals (1) rather than guessing.
  4. "AI-powered" or "AI tools" without further detail — assign signal 1 to the most likely type based on context.
  5. Copilots and AI assistants are typically llm unless explicitly described as autonomous/agentic.
  6. Generic "automation" without AI specificity should get a very low signal (1) across all types.

  ## SUBSTANTIVENESS
  How tangible is the AI adoption disclosure?
  - boilerplate: Pure jargon, no information content. Could appear in any company's report unchanged (e.g., "We leverage AI to drive innovation and improve operations.").
  - moderate: Identifies a specific use case or domain but lacks concrete detail (e.g., "We use AI in our underwriting process." or "We deployed AI in our risk management.").
  - substantive: Names systems, quantifies impact, or explains what/how/why with technical detail (e.g., "We deployed GPT-4 for document review, cutting processing time by 40%.").

  ## SIGNAL GUIDANCE (0-3)
  - 0: not present
  - 1: weak/implicit signal; could be this type but not clearly stated
  - 2: strong implicit signal; type is clearly implied but not explicit
  - 3: explicit, unambiguous type mention

  ## EXAMPLES
  - "We deployed a GPT-based chatbot for HR queries." → llm (signal 3)
  - "We use predictive analytics and machine learning to detect fraud." → non_llm (signal 3)
  - "We are rolling out AI agents to fully automate claims processing end-to-end." → agentic (signal 3), llm (signal 1)
  - "In 2024, our AI platform autonomously handled customer service with no human oversight." → agentic (signal 3), llm (signal 1)
  - "We integrated Microsoft Copilot across our organisation." → llm (signal 3)
  - "We use AI to improve operations." → assign signal 1 to the most likely type based on context 
  - "Our computer vision system detects manufacturing defects on the production line." → non_llm (signal 3)
  - "We have established a committee to navigate the GenAI change and plan" → this only gives us a slight hint of potential implementation of LLMs, but we don't have enough information to determine that the company has any current deployment of LLM systems (signal 0)

  ## OUTPUT CONSTRAINTS
  - Include ALL three adoption_signals entries: one each for non_llm, llm, agentic.
  - Each entry must be an object with fields: {{"type": <label>, "signal": <0-3 integer>}}.
  - substantiveness must be one of: boilerplate, moderate, substantive.

  ## EXCERPT CONTEXT
  Company: {firm_name} | Sector: {sector} | Report Year: {report_year}

risk: |
  You are an expert analyst classifying AI-related risks mentioned in company annual reports.

  ## TASK
  The excerpt has already been classified with the following mention types: {mention_types}.
  Classify the AI related risks (ones that come from or are directly caused by AI) into the categories below. Categories are NOT mutually exclusive; if the excerpt contains multiple risks or a risk spanning multiple categories, include all that apply.
  {reasoning_instruction}

  ## CATEGORIES
  - strategic_competitive: Failure to adopt AI, industry displacement, competitive disadvantage from AI disruption.
  - operational_technical: AI model failures, reliability issues, system errors, hallucinations, accuracy problems.
  - cybersecurity: AI-enabled cyberattacks, AI-related data breaches, adversarial attacks on AI systems, AI-enabled fraud.
  - workforce_impacts: AI-driven job displacement, skills obsolescence, shadow AI usage by employees.
  - regulatory_compliance: AI Act compliance, GDPR/privacy implications of AI, IP/copyright risks, data governance, legal liability from AI decisions, regulatory uncertainty.
  - information_integrity: AI-generated misinformation, deepfakes, content authenticity issues.
  - reputational_ethical: Public trust erosion from AI, ethical concerns, algorithmic bias, human rights implications.
  - third_party_supply_chain: Over-reliance on AI vendors, concentration risk, downstream misuse of AI.
  - environmental_impact: Energy consumption from AI training/inference, carbon footprint, sustainability concerns.
  - national_security: AI in critical infrastructure, defence applications, geopolitical AI risks, export controls.
  - none: The excerpt is too vague to assign a specific risk category, or the risk is not clearly attributable to AI.

  ## RULES
  1. AI must be explicitly or clearly implicitly attributed as the source of the risk. A risk section that mentions both AI and a risk in separate sentences needs a clear causal link to qualify.
  2. Generic risk language without an AI connection is NOT an AI risk — assign "none".
  3. Use "none" when risk language is too vague to assign a category (e.g., "AI can be a risk"). If the excerpt mentions AI and risk separately without expressing risk FROM AI or caused by AI, use "none".

  4. Downstream or indirect risks from AI technologies still count (i.e. "rapid AI adoption creates skills gaps" -> workforce_impacts).
  5. Be conservative. A category gets signal=3 only when the excerpt explicitly states AI is causing that specific category risk in direct language (same sentence/clause-level attribution). If category assignment requires interpretation, use 2 or 1, not 3.
  6. Assign signal scores to reflect how directly the risk category is described, not just whether AI is mentioned.
  7. Do NOT assign a category from nearby context alone. A separate bullet/sentence about generic cyber risk, governance, or ethics does not qualify unless linked to AI risk in the same local statement.
  8. Use strict category assignment: only label a category when the excerpt contains category-specific risk evidence linked to AI; do not infer from generic governance, efficiency, or adjacent risk language.
  9. Output only risk categories that apply. Do not output absent categories.
  10. If "none" is selected, it must be the only risk type.

  ## SUBSTANTIVENESS
  How tangible is the AI risk disclosure?
  - boilerplate: Generic risk language, no information content. Could appear in any company's report unchanged (e.g., "AI poses risks to our business.").
  - moderate: Identifies a specific risk area but no mitigation detail or tangible commitments (e.g., "AI regulation may affect our compliance obligations.").
  - substantive: Describes specific risk mechanisms AND tangible mitigation actions or commitments (e.g., "We allocated EUR 5M to reclassify 3 high-risk AI systems under the EU AI Act by 2025.").

  ## SIGNAL GUIDANCE (1-3)
  - 1: "weak" implicit; category is plausible but uncertain. A conclustion that the company recognizes AI as a potential risk, but does not *directly* name it e.g. "The board created a new standard for AI and Cybersecurity risk."
  - 2: "strong" implicit; not explicitly named, but concrete enough to require interpretation rather than guessing. I.e. "AI is increasing the possibiliy of data breaches and fraud." - it implies that AI is a cyber-risk to the business but does not say so directly.  
  - 3: explicit; clearly stated with no material ambiguity and direct category-level attribution in text. I.e. "AI is a major cybersecurity risk to our business."

  ## EXAMPLES
  - "Increasing risk from new technologies such as Artificial Intelligence (AI) is a major concern." -> none (signal 2, too vague to assign a specific category)
  - "Artificial Intelligence (AI) is posing increasing cybersecurity concerns." -> cybersecurity (signal 3)
  - "[In the Risk Section] Digital threats and cybersecurity risks are evolving rapidly." -> none (signal 3, no AI mention)
  - "We might fail to adopt AI technologies in a timely manner, putting us at a competitive disadvantage." -> strategic_competitive (signal 3)
  - "The increasing regulation, especially the EU AI Act, may result in significant compliance costs and legal liability." -> regulatory_compliance (signal 3)
  - "Deepfakes and GenAI can cause misinformation and damage to our reputation." -> information_integrity (signal 3), reputational_ethical (signal 2)
  - "Rapid advances in AI bring stakeholder scrutiny on data ethics, reskilling and supply chain risks." -> workforce_impacts (signal 2), reputational_ethical (signal 2), third_party_supply_chain (signal 1)
  - "Our AI models could produce inaccurate or unreliable outputs, leading to poor decisions." -> operational_technical (signal 3)
  - "AI is an emrging risk. [...] data privacy is an increasing issue and priority for us" -> regulatory_compliance (signal 1)

  ## OUTPUT CONSTRAINTS
  - risk_types must be non-empty.
  - If "none" is present, it must be the only label.
  - Provide a signal score for EVERY label in risk_types.
  - risk_signals must include exactly one object per risk_types label: {{"type": <risk_type>, "signal": <1-3 integer>}}.
  - Do NOT include risk_signals entries for labels not present in risk_types.
  - Signal scores must be integers: 1=speculative/highly implicit, 2=hard implicit, 3=explicit.
  - substantiveness must be one of: boilerplate, moderate, substantive.

  ## EXCERPT CONTEXT
  Company: {firm_name} | Sector: {sector} | Report Year: {report_year} | Report Section: {report_section}

risk_v2: |
  You are an expert analyst classifying AI-related risk claims in company annual reports.

  ## OBJECTIVE
  We want conservative, high-precision labels.
  Only classify risks that are directly attributed to AI in this excerpt.
  Do NOT maximize recall by adding loosely related tags.

  ## TASK
  The excerpt has already been classified with mention types: {mention_types}.
  Identify AI-caused risks and map them to taxonomy categories.
  Categories are NOT mutually exclusive, but include only those supported by evidence in this excerpt.
  {reasoning_instruction}

  ## AI-RISK CLAIM TEST (MANDATORY FIRST STEP)
  A category can be assigned only if the excerpt states or strongly implies:
  1) an AI capability/system/use, AND
  2) a specific downside/exposure, AND
  3) that downside is caused by AI (directly or clearly via nearby context).

  If this test fails, return:
  - risk_types: ["none"]
  - risk_signals: [{{"type":"none","signal":<1|2|3>}}]

  ## CATEGORIES
  - strategic_competitive: AI-driven competitive disadvantage, disruption, failure to adapt.
  - operational_technical: AI model failure, reliability/accuracy issues, unsafe outputs, model risk.
  - cybersecurity: AI-enabled cyberattacks/fraud, adversarial AI attacks, AI-linked breach exposure.
  - workforce_impacts: AI-driven displacement, skills obsolescence, unsafe employee AI use.
  - regulatory_compliance: AI-specific legal/regulatory/privacy/IP liability or compliance burden.
  - information_integrity: AI-enabled misinformation/deepfakes/content authenticity harms.
  - reputational_ethical: AI-linked trust erosion, fairness/bias/ethics concerns, social license risk.
  - third_party_supply_chain: AI vendor dependency/concentration/downstream third-party AI exposure.
  - environmental_impact: AI energy/carbon/resource burden.
  - national_security: AI-linked geopolitical/security destabilization, critical-system exposure, export-control risk.
  - none: No AI-risk claim present, or too vague to map.

  ## STRICT ATTRIBUTION RULES
  1. Only tag risks caused by AI in the excerpt.
  2. If AI is mentioned in one sentence and generic risk appears elsewhere, do NOT connect them unless the text links them.
  3. Defense/warfare/autonomy context alone is NOT a risk label.
  4. Governance/policy/compliance frameworks alone are NOT risk labels unless downside/liability is stated.
  5. Opportunity/adoption language is NOT risk.
  6. Short/fragmentary chunks (very limited context) should default to conservative scoring; prefer "none" unless attribution is explicit.

  ## SIGNAL GUIDANCE (1-3)
  - 1: weak implicit attribution; plausible but limited textual support.
  - 2: strong implicit attribution; clear enough with some interpretation.
  - 3: explicit attribution; direct statement that AI causes that category risk.

  ## SUBSTANTIVENESS
  - boilerplate: generic risk/governance language; little concrete detail.
  - moderate: specific risk area named, limited mechanism/mitigation detail.
  - substantive: clear risk mechanism and concrete mitigation/actions/commitments.

  ## EXAMPLES (HARD NEGATIVES AND POSITIVES)
  - "We are investing in AI for autonomous defence capabilities..." -> none (signal 3)
  - "AI use in defence must comply with legal frameworks." -> none (signal 3)
  - "AI is increasing phishing and impersonation fraud attempts." -> cybersecurity (signal 3), information_integrity (signal 2)
  - "GenAI outputs may be unreliable; we expanded model governance to mitigate model risk." -> operational_technical (signal 3)
  - "EU AI Act and similar rules may increase compliance costs and legal exposure." -> regulatory_compliance (signal 3)
  - "AI may affect principal risks including cyber, legal, and HR, but impacts remain uncertain." -> use low-signal labels only if AI-causality is stated per risk; otherwise none or sparse 1s
  - "AI is a growing trend... operations may be disrupted by misinformation." -> information_integrity (1) unless direct causal link is stronger
  - "Cyber risk is rising. We also use AI in operations." -> none (no causal linkage)

  ## OUTPUT CONSTRAINTS
  - risk_types must be non-empty.
  - If "none" is present, it must be the only label.
  - Provide exactly one risk_signals entry per label in risk_types.
  - Do NOT include risk_signals for absent labels.
  - Signal scores must be integers in {{1,2,3}}.
  - substantiveness must be one of: boilerplate, moderate, substantive.

  ## EXCERPT CONTEXT
  Company: {firm_name} | Sector: {sector} | Report Year: {report_year} | Report       Section: {report_section}

risk_v3: |
  You are an expert analyst classifying AI-related risks in company annual reports.

  ## OBJECTIVE
  Prioritize precision over recall.
  We want to detect where the company clearly or credibly frames AI as a source of risk, then map that risk into the best-fit taxonomy buckets.

  ## TASK
  The excerpt has already been classified with mention types: {mention_types}.
  Classify AI-caused risks using the buckets below.
  Buckets are broad and can overlap, but include only those supported by this excerpt.
  {reasoning_instruction}

  ## CORE PRINCIPLE (ATTRIBUTION FIRST)
  Assign a risk label only when the text links AI to a downside/exposure.
  If risk language is present but not attributable to AI in the excerpt, do not label that category.
  If there is no attributable AI-risk in the excerpt, use "none".

  ## RISK BUCKETS
  - strategic_competitive: AI changes market structure, customer behavior, pricing power, or competitive position.
  - operational_technical: AI quality/reliability/model-risk issues (errors, instability, unsafe outputs, decision-quality degradation).
  - cybersecurity: AI-linked attack or defense exposure (fraud, impersonation, adversarial abuse, breach pathways).
  - workforce_impacts: AI-driven workforce transition risk (skills gaps, displacement pressure, unsafe employee AI usage).
  - regulatory_compliance: AI-linked legal/regulatory/privacy/IP exposure, compliance burden, or liability.
  - information_integrity: AI-enabled misinformation, deepfakes, authenticity erosion, manipulation risk.
  - reputational_ethical: AI-related trust, fairness, ethics, or stakeholder legitimacy concerns.
  - third_party_supply_chain: Dependence on external AI vendors, concentration risk, or third-party AI misuse exposure.
  - environmental_impact: AI energy/carbon/resource-intensity risk.
  - national_security: AI-linked geopolitical/security instability, critical-system vulnerability, export-control/security-of-state concerns.
  - none: no attributable AI-risk claim in this excerpt, or too vague to map reliably.

  ## DECISION RULES
  1. Keep category assignment local to the excerpt. Do not infer from likely broader context.
  2. AI mention + separate generic risk mention is not enough unless the text links them.
  3. Defense/autonomy/warfare capability language alone is not a risk category.
  4. Governance/policy/compliance framework language alone is not a risk category unless downside/exposure is stated.
  5. Use conservative labeling for short or fragmented chunks; prefer lower signals or "none" when uncertain.
  6. Do not add categories just because they are adjacent or plausible.

  ## SIGNAL GUIDANCE (1-3)
  - 1: weak implicit attribution (plausible but lightly supported in text)
  - 2: strong implicit attribution (clear but still requires interpretation)
  - 3: explicit attribution (text directly states AI causes that category risk)

  ## SUBSTANTIVENESS
  - boilerplate: generic risk/governance language; little concrete mechanism.
  - moderate: identifies a specific AI-risk area, limited mechanism or mitigation detail.
  - substantive: clear AI-risk mechanism and concrete mitigation/actions/commitments.

  ## EXAMPLES
  - "We are investing in AI for autonomous defence capabilities." -> none (signal 3)
  - "AI systems in defence must comply with legal frameworks." -> none (signal 3)
  - "AI-generated impersonation is increasing fraud attempts against clients." -> cybersecurity (3), information_integrity (2)
  - "GenAI outputs may be unreliable in decision workflows; we expanded model controls." -> operational_technical (3)
  - "The EU AI Act may increase compliance burden and legal exposure for our AI systems." -> regulatory_compliance (3)
  - "AI may affect cyber and HR risks, but impacts remain uncertain." -> use sparse low-signal labels only where AI attribution is present; otherwise none
  - "AI is a growing trend. Operations can be disrupted by misinformation." -> information_integrity (1) unless attribution is made more direct
  - "Cyber threats are increasing. We also use AI in operations." -> none

  ## OUTPUT CONSTRAINTS
  - risk_types must be non-empty.
  - If "none" is present, it must be the only label.
  - Provide exactly one risk_signals entry per label in risk_types.
  - Do NOT include risk_signals entries for labels not in risk_types.
  - Signal scores must be integers in {{1,2,3}}.
  - substantiveness must be one of: boilerplate, moderate, substantive.

  ## EXCERPT CONTEXT
  Company: {firm_name} | Sector: {sector} | Report Year: {report_year} | Report Section: {report_section}

risk_v4: |
  You are an expert analyst classifying AI-risk disclosures in company annual reports.

  ## OBJECTIVE
  Maximize precision for AI-risk tagging.
  Tag only what the company explicitly or credibly acknowledges as AI-caused downside exposure.
  Do not convert AI opportunity/adoption/governance language into risk labels without clear downside evidence.

  ## TASK
  The excerpt has already been classified with mention types: {mention_types}.
  Identify AI-caused risks and map them to the taxonomy below.
  Include only categories directly supported by the excerpt.
  {reasoning_instruction}

  ## HARD GATE (MUST PASS BEFORE ANY RISK LABEL)
  A non-"none" risk label is allowed only if ALL three are present in the excerpt:
  1) AI anchor: explicit AI/GenAI/ML/model/system reference.
  2) Downside anchor: explicit exposure/harm/liability/failure mention (not just "change", "transform", "trend", or "uncertainty").
  3) Causal link: the downside is attributed to AI (directly or in tightly linked local context).

  If any anchor is missing, output:
  - risk_types: ["none"]
  - risk_signals: [{{"type":"none","signal":3}}]

  ## RISK TAXONOMY
  - strategic_competitive: AI-driven competitive disadvantage, displacement, pricing/margin pressure, strategic erosion from AI.
  - operational_technical: AI reliability/accuracy/safety/model-risk failures that could degrade decisions or operations.
  - cybersecurity: AI-enabled attacks/fraud/breach pathways/adversarial abuse.
  - workforce_impacts: AI-driven displacement pressure, skills gaps, or unsafe employee AI usage.
  - regulatory_compliance: AI-specific legal/regulatory/privacy/IP liability, compliance burden, or enforcement exposure.
  - information_integrity: AI-enabled misinformation/deepfakes/authenticity manipulation.
  - reputational_ethical: AI-linked trust/ethics/fairness/human-rights legitimacy risk.
  - third_party_supply_chain: dependency/concentration/downstream exposure from external AI vendors/providers.
  - environmental_impact: AI-related energy/carbon/resource burden framed as downside risk.
  - national_security: AI-linked geopolitical/security destabilization or critical-systems exposure.
  - none: no attributable AI-risk claim in this excerpt, or evidence too weak/vague to map.

  ## DISQUALIFIERS (DEFAULT TO "none")
  1. Pure opportunity/adoption language ("transform", "improve", "enable", "megatrend") without explicit downside.
  2. Governance/process language alone (roadmaps, committees, principles, frameworks, strategy integration) without stated risk mechanism or liability.
  3. AI mention and generic enterprise risk in separate statements with no explicit connection.
  4. High-level "AI may pose risks" with no category-specific evidence; use "none" unless a concrete risk mechanism is named.
  5. Short/fragmentary text where attribution is uncertain; prefer "none".

  ## CATEGORY DECISION RULES
  1. Keep mapping local to this excerpt only; do not import assumptions from broader report context.
  2. Assign the minimum number of categories required by textual evidence.
  3. Do not assign adjacent/plausible categories unless each has direct support.
  4. If multiple categories are truly evidenced, include all supported categories.
  5. If "none" is selected, it must be the only risk type.

  ## SIGNAL GUIDANCE (1-3)
  - 1: weak implicit AI-to-risk attribution; limited textual support.
  - 2: strong implicit attribution; clear but still inferential.
  - 3: explicit attribution; excerpt directly states AI causes that category risk.

  ## SUBSTANTIVENESS
  - boilerplate: generic AI-risk language; no specific mechanism or concrete action.
  - moderate: specific AI-risk area identified but limited mechanism or mitigation detail.
  - substantive: specific AI-risk mechanism plus tangible mitigation actions/commitments.

  ## EXAMPLES (PRECISION-ORIENTED)
  - "AI and digitalization are long-term megatrends; we updated technology roadmaps." -> none (3)
  - "AI may affect our business; we monitor developments." -> none (3)
  - "We use AI and face growing cyber threats." -> none (3) unless AI is linked to the threat mechanism
  - "GenAI-enabled impersonation is increasing fraud attempts against our clients." -> cybersecurity (3), information_integrity (2)
  - "Our AI models can produce unreliable outputs in credit decisions." -> operational_technical (3)
  - "EU AI Act obligations may increase compliance costs and liability for our AI systems." -> regulatory_compliance (3)
  - "Dependence on a small number of AI model providers could disrupt service continuity." -> third_party_supply_chain (3)

  ## OUTPUT CONSTRAINTS
  - risk_types must be non-empty.
  - If "none" is present, it must be the only label.
  - Provide exactly one risk_signals entry per label in risk_types.
  - Do NOT include risk_signals entries for labels not in risk_types.
  - Signal scores must be integers in {{1,2,3}}.
  - substantiveness must be one of: boilerplate, moderate, substantive.

  ## EXCERPT CONTEXT
  Company: {firm_name} | Sector: {sector} | Report Year: {report_year} | Report Section: {report_section}

risk_v5: |
  You are an expert analyst classifying AI-related risks in company annual reports.

  ## OBJECTIVE
  Classify AI-caused or AI-linked risks with calibrated signal strength.
  Use the full signal range (1-3) to capture how directly AI is attributed as the risk source.
  Prioritize precision: do not invent risk categories the text does not support.
  But do not discard plausible risk signals — use low signal scores (1) for indirect or implicit mentions rather than omitting them entirely.

  ## TASK
  The excerpt has already been classified with mention types: {mention_types}.
  Identify risks that are caused by, linked to, or exacerbated by AI, and map them to the taxonomy below.
  Categories are NOT mutually exclusive; include all that are supported by the excerpt.
  {{reasoning_instruction}}

  ## ATTRIBUTION FRAMEWORK (MAPS DIRECTLY TO SIGNAL SCORES)
  Before assigning any risk category, assess the strength of AI-to-risk attribution:

  Signal 3 — EXPLICIT: The text directly states that AI causes, creates, or exposes the company to this specific category of risk. AI and the downside appear in the same statement with clear causal language.
    Example: "AI-generated deepfakes are increasing fraud attempts against our clients." -> cybersecurity (3)

  Signal 2 — STRONG IMPLICIT: AI and a specific downside are both clearly present and linked, but the causal connection requires modest interpretation. The reader can confidently infer the risk category.
    Example: "AI is increasing the possibility of data breaches and fraud." -> cybersecurity (2)

  Signal 1 — WEAK IMPLICIT: AI is mentioned in a risk context and a specific risk category is plausible, but the attribution is indirect or requires significant interpretation. The text hints at the risk rather than stating it.
    Example: "The board established new governance standards for AI and cybersecurity risk." -> cybersecurity (1)

  None — NO ATTRIBUTION: AI is not mentioned in connection with risk, or the excerpt contains only generic risk language, or only opportunity/adoption/governance language without any downside framing.

  ## RISK TAXONOMY
  - strategic_competitive: AI-driven competitive disadvantage, displacement, failure to adapt, pricing/margin erosion.
  - operational_technical: AI reliability/accuracy/safety/model-risk failures that degrade decisions or operations.
  - cybersecurity: AI-enabled attacks, fraud, breach pathways, adversarial abuse of AI systems.
  - workforce_impacts: AI-driven displacement, skills gaps, unsafe employee AI usage (shadow AI).
  - regulatory_compliance: AI-specific legal/regulatory/privacy/IP liability, compliance burden, enforcement exposure.
  - information_integrity: AI-enabled misinformation, deepfakes, content authenticity manipulation.
  - reputational_ethical: AI-linked trust erosion, fairness/bias/ethics concerns, human rights implications.
  - third_party_supply_chain: Dependency on AI vendors, concentration risk, downstream AI misuse exposure.
  - environmental_impact: AI energy/carbon/resource burden framed as downside risk.
  - national_security: AI-linked geopolitical/security destabilization, critical-systems exposure.
  - none: No attributable AI-risk in this excerpt, or attribution too weak even for signal 1.

  ## DECISION RULES
  1. If the excerpt vaguely connects AI to a risk area, assign the relevant category with signal 1 rather than omitting it. Reserve "none" for excerpts with genuinely no AI-risk content.
  2. Keep category assignment local to this excerpt; do not import assumptions from broader report context.
  3. Include categories at low signal when there is indirect support rather than dropping them — but do not add categories with no textual basis.
  4. Opportunity/adoption language alone is NOT risk. But if the same excerpt frames AI adoption as creating exposure (e.g., "rapid AI adoption creates skills gaps"), that IS risk.
  5. Governance/framework language alone is NOT risk at signal 3. But if governance is mentioned in response to a stated AI risk concern, assign the relevant category at signal 1 or 2.
  6. AI mention + separate generic risk mention with no textual link between them = "none".
  7. If "none" is selected, it must be the only risk type.
  8. For short/fragmentary chunks where AI-risk attribution is plausible but uncertain, prefer signal 1 over "none".

  ## SUBSTANTIVENESS
  - boilerplate: Generic AI-risk language; no specific mechanism or concrete action.
  - moderate: Specific AI-risk area identified but limited mechanism or mitigation detail.
  - substantive: Specific AI-risk mechanism plus tangible mitigation actions/commitments.

  ## EXAMPLES (SIGNAL-CALIBRATED)
  - "AI and digitalization are long-term megatrends; we updated technology roadmaps." -> none (no downside framing)
  - "AI may affect our business; we monitor developments." -> none (too vague, no specific risk category)
  - "We use AI and face growing cyber threats." -> none (no causal link between AI and the cyber threats)
  - "Increasing risk from new technologies such as AI is a major concern." -> none (too vague to assign a specific category despite downside framing)
  - "AI is an emerging risk. Data privacy is an increasing priority for us." -> regulatory_compliance (1); AI flagged as risk, privacy concern nearby but link is indirect
  - "The board created a new standard for AI and cybersecurity risk." -> cybersecurity (1); co-occurrence in governance context, no causal claim
  - "Rapid advances in AI bring stakeholder scrutiny on data ethics, reskilling, and supply chain risks." -> workforce_impacts (2), reputational_ethical (2), third_party_supply_chain (1)
  - "AI is increasing the possibility of data breaches and fraud." -> cybersecurity (2); implies AI-caused cyber risk without stating it directly
  - "GenAI-enabled impersonation is increasing fraud attempts against our clients." -> cybersecurity (3), information_integrity (2)
  - "Our AI models can produce unreliable outputs in credit decisions." -> operational_technical (3)
  - "EU AI Act obligations may increase compliance costs and liability for our AI systems." -> regulatory_compliance (3)
  - "Dependence on a small number of AI model providers could disrupt service continuity." -> third_party_supply_chain (3)
  - "We might fail to adopt AI in a timely manner, putting us at a competitive disadvantage." -> strategic_competitive (3)
  - "Deepfakes and GenAI can cause misinformation and damage to our reputation." -> information_integrity (3), reputational_ethical (2)
  - "GenAI outputs may be unreliable; we expanded model governance to mitigate model risk." -> operational_technical (3)
  - "AI may affect principal risks including cyber, legal, and HR, but impacts remain uncertain." -> cybersecurity (1), regulatory_compliance (1), workforce_impacts (1); AI attribution stated per risk area but uncertain

  ## OUTPUT CONSTRAINTS
  - risk_types must be non-empty.
  - If "none" is present, it must be the only label.
  - Provide exactly one risk_signals entry per label in risk_types.
  - Do NOT include risk_signals entries for labels not in risk_types.
  - Signal scores must be integers in {{1,2,3}}.
  - substantiveness must be one of: boilerplate, moderate, substantive.

  ## EXCERPT CONTEXT
  Company: {firm_name} | Sector: {sector} | Report Year: {report_year} | Report Section: {report_section}

substantiveness: |
  You are an expert analyst assessing the substantiveness of AI disclosures in company annual reports.

  ## TASK
  The excerpt has been classified to have the following mention types: {mention_types}.
  Classify the AI-related content as boilerplate, contextual, or substantive.

  ## EXAMPLES
  - "We use AI to improve our customer service." -> substantive ~0.9
  - "We are exploring AI opportunities." -> boilerplate ~0.8
  - "AI may pose risks to our industry." -> contextual ~0.7
  - "We deployed GPT-4 for automated document review, reducing processing time by 40%." -> substantive ~0.95

  ## EXCERPT CONTEXT
  Company: {firm_name} | Sector: {sector} | Report Year: {report_year} | Report Section: {report_section}

vendor: |

  ## UPDATED PROMPT
  You are an expert analyst extracting AI vendor mentions from company annual reports.

  ## TASK
  The excerpt has already been classified with the following mention types: {mention_types}.
  Identify which AI vendors/providers are used by the company to deploy AI capabilities.
  The objective is to detect the underlying AI model (LLM or agentic AI) or model platform (if any).
  Tags are NOT mutually exclusive.
  Make sure to assign an appropriate signal strength to each vendor mentioned.
  {reasoning_instruction}

  ## VENDOR TAGS
  - amazon: Amazon, AWS, Bedrock, SageMaker, Amazon Q, CodeWhisperer, Titan
  - google: Google, Vertex AI, Gemini, DeepMind, Google Cloud AI
  - microsoft: Microsoft Azure AI, Copilot, Azure OpenAI Service
    (note: "Microsoft Teams" or "Microsoft Office" alone is NOT an AI vendor mention)
  - openai: OpenAI, GPT, ChatGPT
  - anthropic: Anthropic, Claude
  - meta: Meta AI, Llama
  - internal: Explicitly stated in-house / proprietary AI development. Only use this tag if the company explicitly implies internal development or deployment of AI models.
  - undisclosed: Third-party AI vendor clearly mentioned but the name is deliberately not mentioned.
  - other: Named AI vendor not in the above list (specify in other_vendor field). These might include startups or smaller AI companies such as i.e. Cohere AI or Mistral AI.

  ## RULES
  1. Tag only vendors that provide AI models or AI model platforms to the company.
    - Ignore name-drops in bios, industry commentary, or events.
    - Ignore general cloud, productivity, or partnership mentions without AI model usage.
  2. Model-name mentions count as vendor signals even if the company name is absent:
    - GPT: openai
    - Claude: anthropic
    - Gemini / Vertex: google
    - Llama: meta
    - Bedrock / Titan / CodeWhisperer: amazon
  3. If a product clearly involves two vendors (e.g., Azure OpenAI Service), tag both.
  4. If AI is used but the vendor is unnamed, tag "undisclosed".
  5. If the company states it builds AI internally, tag "internal". Only use this tag if the company explicitly implies internal development AI base models.

  ## SIGNAL GUIDANCE (integer: 1, 2, or 3)
  Assign a signal strength for each vendor in vendor_tags:
  - 1 (weak implicit): Requires interpretation
    Example: "Our AI models are from a company that inveted the AI transformer technology"
  - 2 (strong implicit): Clearly about this vendor but not directly stated
    Example: “We are enhancing operations with Copilot tools”
  - 3 (explicit): Directly and factually evident
    Example: “We deployed OpenAI's GPT-4 for document review”

  ## EXAMPLES
  - "We deployed OpenAI's GPT-4 for document review." -> vendors: [{{vendor: "openai", signal: 3}}]
  - "We use Azure OpenAI Service for internal copilots." -> vendors: [{{vendor: "microsoft", signal: 3}}, {{vendor: "openai", signal: 3}}]
  - "Our team built a proprietary ML platform in-house." -> vendors: [{{vendor: "internal", signal: 3}}]
  - "We use a third-party AI solution for fraud detection." -> vendors: [{{vendor: "undisclosed", signal: 2}}]
  - "We use Microsoft 365 for productivity." -> vendors: [] (not an AI vendor mention)

  ## OUTPUT CONSTRAINTS
  - If no AI vendor/model provider is detected, vendors must be an empty list. Make sure to do this in the case of false positives, where the excerpt has falsly been classified as having AI / AI vendor mentions.
  - Each entry in vendors must have a vendor tag and a signal score (1, 2, or 3).
  - No duplicate vendor tags in the vendors list.
  - Set other_vendor only if "other" is included as a vendor tag in the vendors list.

  ## EXCERPT CONTEXT
  Company: {firm_name} | Sector: {sector} | Report Year: {report_year}
